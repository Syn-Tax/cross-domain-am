---
bibliography: [../../Cross-Domain AM.bib]
---

# Introduction

Argument relation identification (ARI) is a subtask of argument mining (AM) involved with the detection and classification of the argumentative components contained within text and discussion [@lawrenceArgumentMiningSurvey2020]. AM generally can cover many different tasks, including argumentative sequence detection, argument component classification and ARI in a specific domain. All of these tasks (often when combined) have many different applications such as fake news [@kotonyaGradualArgumentationEvaluation2019] and fallacy detection [@manciniMultimodalFallacyClassification2024] with uses across many fields including political science, military and corporate intelligence, and education. In all of these fields, it is important to be able to extract arguments and their relations accurately and with ease.

In many contexts (e.g. political debates or court proceedings) both the audio and textual data exist, where typically the text has been transcribed from the audio. Previously, most argument mining systems have focused completely on the textual data, often gained from a transcript of these discussions or debates. More recent research, however, has shown that the paralinguistic features available from other modalities of data (either images or audio) are able to improve the performance over text only solutions [@manciniMultimodalArgumentMining2022].

Multimodal models have shown the greatest level of improvement in argumentative sentence detection and argument segmentation [@ruiz-dolzVivesDebateSpeechCorpusSpoken2023]. However, they have only had varying degrees of success in ARI with Mancini *et al.* [@manciniMAMKitComprehensiveMultimodal2024] finding the addition of acoustic features significantly increased performance while Mestre *et al.* [@mestreMArgMultimodalArgument2021] find no significant difference. Recent advances in multimodal models and their pre-training have now allowed the effective addition of acoustic features [@schneiderWav2vecUnsupervisedPretraining2019;@chenWavLMLargeScaleSelfSupervised2022], along with advances in other areas of natural language processing, more specifically, the advent of the transformer [@vaswaniAttentionAllYou2017] and the recent boom of large language models (LLMs), including their use in text-based argument mining [@gorurCanLargeLanguage2024]. Due to their size, LLMs perform very well across a variety of domains and fields, but at great computational cost. Therefore, it is pertinent to understand how smaller models perform across a number of domains.

Traditionally, AM systems have been evaluated on the same domain that they were trained on [@haddadanYesWeCan2019;@egerNeuralEndtoEndLearning2017]. But given the recent uses of relatively task agnostic pre-training approaches and models such as BERT [@devlinBERTPretrainingDeep2019b] and then the advent of LLMs has further increased the need for domain agnostic models. These advancements have also been applied to argument mining by evaluating models on datasets and domains with which they were not trained  [@ruiz-dolzTransformerBasedModelsAutomatic2021;@gemechuARIESGeneralBenchmark2024]. This approach has not yet been evaluated on multimodal models generally nor text-audio models specifically, in any sub-task of argument mining, it is very possible that the addition of acoustic features will be helpful for the model to learn more intrinsic, less domain-specific features and therefore increase its ability to generalise beyond the domain in which it was trained. A likely reason that this has not been studied further is the relative lack of resources and available datasets designed for text-audio ARI. The only datasets currently available for the task are the M-Arg political debate dataset [@mestreMArgMultimodalArgument2021] with 7 hours of audio data, and the VivesDebate-Speech dataset [@ruiz-dolzVivesDebateSpeechCorpusSpoken2023] with 12 hours of audio data. Although they are the largest text-audio datasets of their type which are currently available, when comparing to the large text only datasets they are still relatively small. For example, M-Arg and Vivesdebate-Speech contain 6,527 and 7,810 nodes with one of the largest debate datasets, QT30 [@hautli-janiszQT30CorpusArgument2022] containing almost 20,000. This difference in size means that there is still significant room for multimodal ARI datasets to grow and hints at a limitation surrounding the current approaches and models.

It is for this reason that the goals of this project are twofold:

(i) Extend two existing debate corpora with their audio data, allowing their use for all multimodal or audio only AM applications. The first of these is the large QT30 corpus [@hautli-janiszQT30CorpusArgument2022] and the second is a smaller, but cross-domain corpus created from Moral Maze episodes [@lawrenceAIFdbCorpora2014].
(ii) The implementation and evaluation of different multimodal techniques and models in a cross-domain setting on the task of argument relation identification.
