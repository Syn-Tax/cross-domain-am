---
bibliography: [../../Cross-Domain AM.bib]
---

# Introduction

Argument relation identification (ARI) is a subtask of argument mining (AM) involved with the detection and classification of the argumentative components contained within text and discussion [@lawrenceArgumentMiningSurvey2020]. AM generally can cover many different tasks, including argumentative sequence detection, argument component classification and ARI in a specific domain.

All of these tasks (often when combined) have many different applications such as fake news and fallacy detection with uses across many fields including political science, military and corporate intelligence, and education. In all of these fields it is important to be able to extract arguments and their relations accurately and with ease.

In many contexts (e.g. political debates or court proceedings) there is both the audio data which can then be transcribed into the textual data. Previously, most argument mining systems have focused completely on the textual data, often gained from a transcript of these discussions or debates. More recent research, however, has shown that the paralinguistic features available from other modalities of data (either images or audio) are able to improve the performance over text only solutions.

Multimodal models have shown the greatest level of improvement in argumentative sentence detection and argument segmentation [@ruiz-dolzVivesDebateSpeechCorpusSpoken2023]. However, they have only had varying degrees of success in ARI with Mancini *et al.* [@manciniMAMKitComprehensiveMultimodal2024] finding the addition of acoustic features significantly increased performance while Mestre *et al.* [@mestreMArgMultimodalArgument2021] find no significant difference. Recent advances in multimodal models and their pre-training have now allowed the effective addition of acoustic features [@schneiderWav2vecUnsupervisedPretraining2019;@chenWavLMLargeScaleSelfSupervised2022], along with advances in other areas of natural language processing, more specifically, the advent of the transformer [@vaswaniAttentionAllYou2017] and the recent boom of large language models (LLMs), including their use in text-based argument mining [@gorurCanLargeLanguage2024]. Due to their size, LLMs perform very well across a variety of domains and fields, but at great computational cost. Therefore it is pertinent to understand how smaller models perform across a number of domains.

Traditionally, argument mining systems have been evaluated on the same domain that they were trained on [@haddadanYesWeCan2019;@egerNeuralEndtoEndLearning2017]. But given the recent uses of relatively task agnostic pre-training approaches and models such as BERT [@devlinBERTPretrainingDeep2019b] and then the advent of LLMs has further increased the need for domain agnostic models. These advancements have also been applied to argument mining by evaluating models on datasets and domains with which they were not trained  [@ruiz-dolzTransformerBasedModelsAutomatic2021;@gemechuARIESGeneralBenchmark2024]. This approach has not yet been evaluated on multimodal models generally nor text-audio models specifically, in any sub-task of argument mining, it is very possible that the addition of acoustic features will be helpful for the model to learn more intrinsic, less domain-specific features and therefore increase its ability to generalise beyond the domain in which it was trained. A likely reason that this has not been studied further is the relative lack of resources and available datasets designed for text-audio ARI. The only dataset currently available for the task is the M-Arg policital debate dataset [@mestreMArgMultimodalArgument2021].

It is for this reason that the goals of this project are twofold:

(i) Extend two existing debate datasets with their audio data allowing their use for all multimodal or audio-only AM applications. The first of these is the large QT30 corpus [@hautli-janiszQT30CorpusArgument2022] and the second is a smaller, but cross-domain corpus created from Moral Maze episodes [@lawrenceAIFdbCorpora2014].
(ii) Perform an extensive evaluation of different multimodal techniques and models in a cross-domain setting on the task of argument relation identification.